{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Overview .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itSammycodethngy/ApiCook/blob/master/RNN_Overview_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMCxdJfhwIcJ"
      },
      "source": [
        "<h4> Recurrent Neural Network Overview </h4>\n",
        "In Simple put all other neural networks do not have memory and so they cannot deal with sequential data . For example the position of a word in a sentence would have significant impact on the its meaning . RNN learn representation of sequences in which order is important . They need to have a memory of the past , anytime we feed a word as input , we alsop feed a state variable which has knowledge of the past. In RNN we have an additional matrix that helps learn the past state . The state vector at the end of inputting a full sentence is a representation of the sentence taking into account of the order of input . \n",
        "\n",
        "<h6> LSTM </h6>\n",
        "Special kind of RNN that can learn long  term depencies . Cell state carries information around the network , thus it has the ability to carry infomation from one end to another .It can be called the conveyor belt. \n",
        "LSTMs have the ability to remove and add information to the cell state , carefully regulated by structures called gates . They are a way to optionally allow an information through . The sigmoid layer outputs number between 0-1 showing exactly how much information should be read through .<br>\n",
        "\n",
        "1. First step is to decide which information to throw away from the cell state , this will be done by a signoid gate known as the forget gate \n",
        "\n",
        "2. The second step is to decide what new information we are we going to store in the cell state . A sigmoid gate decides which values need to be update , then a tan layer prepare candidate vectors that could be added to the state \n",
        "\n",
        "3. Deciding the output , first we run a sigmoid gate which decide what part of the cell state to output . Then we put the cell state through a tan function and multiply by the output of the sigmoid gate , so the network only output the part it has decided to .\n",
        "\n",
        "<h6> GRU (Gated Recurrent Unit) </h6> \n",
        "The GRU merges the input , forget gate into one single update gate . It also merges the cell state and the hidden state , the resultant model is faster and computationally less expensive , LSTM are better in performance but much more expensive . Choosing GRU is a trade-off between computation and performance . \n",
        "\n",
        "<h6> Applications </h6> \n",
        "1. Natural language Processing <br>\n",
        "2. Machine Translation <br>\n",
        "3. Image Captioning <br>\n",
        "4. Speech Recognition <br>\n",
        "5. Sentiment analysis \n",
        "\n",
        "\n",
        "<h2> Data Pre-Processing </h2>\n",
        "How keras helps in pre processing . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0j9QxVlwIcL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0f57de3-4de8-4673-e4d9-34643662a3c2"
      },
      "source": [
        "import os \n",
        "import numpy as np \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P8IUeOPwIcS"
      },
      "source": [
        "<h4> Load Data </h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T59bdihCwIcT"
      },
      "source": [
        "def load_file(path):\n",
        "    input_file = os.path.join(path)\n",
        "    with open (input_file, \"r\", encoding =\"utf8\") as f:\n",
        "        data = f.read()\n",
        "        \n",
        "    return data.split('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKmBrvAzwIcX"
      },
      "source": [
        "english_sentences = load_file('train.en.txt')\n",
        "vietnam_sentences = load_file('train.vi.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76MsBbTxwIcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "ceb34c49-3135-4f60-ac3c-acf1527586e7"
      },
      "source": [
        "print(english_sentences[1])\n",
        "print(vietnam_sentences[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
            "Trong 4 phút , chuyên gia hoá học khí quyển Rachel Pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y-e9uyQwIci"
      },
      "source": [
        "<h4> Tokenizer </h4>\n",
        "As you might have probably guessed , a neural network cannot work with text data . It needs numbers which it can perform computations on \n",
        "We will therefore turn each word in the sentence into numbers .  The Tokenizer feature of the keras framework will help us .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx3uoc14wIcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "cd17ce0c-7a3b-4c99-851f-a7aaaa121f61"
      },
      "source": [
        "def tokenize(input):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(input)\n",
        "    input_tokenized = tokenizer.texts_to_sequences(input)\n",
        "    \n",
        "    return input_tokenized , tokenizer\n",
        "\n",
        "english_data_tokenized , english_tokenizer = tokenize(english_sentences)\n",
        "vietnam_data_tokenized , vietnam_tokenizer = tokenize(vietnam_sentences)\n",
        "\n",
        "print(english_data_tokenized[1])\n",
        "print(vietnam_data_tokenized[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9, 1297, 448, 8298, 6152, 6343, 15813, 3291, 6, 4182, 5, 1, 1226, 901, 1464, 555, 1, 2705, 3608, 25, 670, 159, 26, 131, 559, 38, 5, 508, 64, 5226, 439, 6, 5227, 1692, 119, 1, 3749, 9, 3680, 5, 206, 25, 6, 707, 1438]\n",
            "[14, 588, 574, 619, 145, 179, 60, 391, 917, 3380, 7329, 89, 945, 951, 1187, 32, 7, 1133, 256, 293, 60, 3381, 2864, 1062, 102, 7, 354, 110, 1300, 835, 32, 294, 142, 391, 689, 117, 20, 1165, 266, 209, 8, 82, 162, 854, 13, 18, 1969, 1335, 22, 314, 431, 17, 4, 831, 428, 1320, 610, 133, 872, 907, 28, 146, 491, 144, 125, 32, 4, 283, 358, 2427, 1295]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDfZCBAxwIco"
      },
      "source": [
        "<h4> Add Padding </h4>\n",
        "In order for our model to train efficiently , each sequence needs to be of the same rank , since sentences are different in ranks , we can add zeros to the ones with small length so we can have the same length. The padding_sequence will automate the process for us , we can specify where we want to add the zeros , at the beginnig or at the end . This will not affect the performance in anyway. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1LFOpwbwIcq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "be8ae6f3-b00b-4157-d776-7a400672c370"
      },
      "source": [
        "def pad(input, length=None):\n",
        "    if length == None:\n",
        "        length = max([len(seq) for seq in input])\n",
        "        \n",
        "    return pad_sequences(input, maxlen = length, padding='post')\n",
        "\n",
        "english_data_padded = pad(english_data_tokenized)\n",
        "# We need to reshape so we can use s sparse categorical function entropy. \n",
        "vietnam_data_padded = pad(vietnam_data_tokenized)\n",
        "vietnam_data_padded = vietnam_data_padded.reshape(*vietnam_data_padded.shape,1)\n",
        "\n",
        "print(english_data_padded.shape)\n",
        "print (vietnam_data_padded.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(133318, 612)\n",
            "(133318, 720, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9o6MOrxwIcu"
      },
      "source": [
        "<h4> Build Model </h4>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaxfxPnMwIcu"
      },
      "source": [
        "from keras.layers import GRU, Input, Dense , TimeDistributed \n",
        "from keras.models import Model , Sequential \n",
        "from keras.layers import Activation \n",
        "from keras.optimizers import Adam \n",
        "from keras.losses import sparse_categorical_crossentropy \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25kDnNjBwIcx"
      },
      "source": [
        "<h4>About Time Distributed</h4>\n",
        "We wrap the Dense layer in a time distributed function . What this does is to apply dense layer to every time step , so instead of applying a dense layer to a whole sentence , the network will apply the dense layer to each word in the sentence , we are doing this in order to keep each time step value separate or predict each word based on previous once intead of predicting a whole sentence . This will make our predictions more accurate . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAIGPxuywIcy"
      },
      "source": [
        "def simple_model(input_shape, output_len, num_uniq_en_words, num_uniq_vi_words):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=256, input_shape = input_shape[1:], return_sequences=True))# You can use LSTM too \n",
        "    model.add(TimeDistributed(Dense(num_uniq_vi_words))) # We set a dense layer to the number of vietnam words so the model can predict how likely each word is . \n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    learning_rate = 0.002\n",
        "    \n",
        "    model.compile(loss =spare_categorical_crossentropy, optimizer = Adam(learning_rate), metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNLxEVnPwIc0"
      },
      "source": [
        "<h4> Build Advanced Model</h4>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvJNi8VFwIc1"
      },
      "source": [
        "from keras.layers import Embedding, Bidirectional, RepeatVector, Flatten\n",
        "from keras.layers import LSTM\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31GxdSxpwIc4"
      },
      "source": [
        "<h4> Approach </h4>\n",
        "First we add an <b>embedding layer</b> , it will compress our data from sparse vectors to non sparse vectors . It will also merge <br>\n",
        "Similar words into similar regions in the vector space . It will learn the retionship between words . The other layers will also take the same input size as the embedding layer since it's output will serve as their input, we can see that they are all 512 in size . The <b> Bidirectional layer </b> simply make the network  learn from the future of the sentences  and not only the past ones . They will help increase the performance of the network . Because the network is bidirectional , we need to generate the word from the whole sentence . This is the work of the <b> Repeat Vector</b> it will repeat the whole sentence for as input for each word we predict , that means each timestep will get the same input but different hidden state , if we do not do that , we will only get one vietnam word per english sentence .  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c76lS_XbwIc4"
      },
      "source": [
        "def advanced_model(input_shape, output_len, num_uniq_en_words, num_uniq_vi_words):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(num_uniq_en_words, 512, input_length = input_shape[1]))\n",
        "    model.add(Bidirectional(LSTM(512, return_sequences= False)))\n",
        "    model.add(RepeatVector(output_len))\n",
        "    model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(num_uniq_vi_words)))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    learning_rate = 0.002\n",
        "    \n",
        "    model.compile(loss =sparse_categorical_crossentropy, optimizer = Adam(learning_rate), metrics=['accuracy'])\n",
        "   \n",
        "    \n",
        "    return model \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3zXoNzqwIc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "caad924e-50f8-4de4-804f-5ed8171c962e"
      },
      "source": [
        "model = advanced_model(english_data_padded.shape, vietnam_data_padded.shape[1],\n",
        "                      len(english_tokenizer.word_index), len(vietnam_tokenizer.word_index))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQ2BXl3wIc7"
      },
      "source": [
        "<h4> Train the Model </h4>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKm45CCMwIc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "c1e3c497-14d3-46bd-8be7-58d5e10b3c69"
      },
      "source": [
        "history = model.fit(english_data_padded[:100], vietnam_data_padded[:100], batch_size=64, epochs =3 , validation_split=0.1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 90 samples, validate on 10 samples\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzhu7095wIc_"
      },
      "source": [
        "vietnam_data_padded.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wIsMS-owIdC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}